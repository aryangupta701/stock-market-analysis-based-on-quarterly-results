{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd20ece6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d242768",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"C:\\\\Users\\\\garya\\\\ML projects\\\\Stock analysis\\\\modelling\\\\dataset\\\\deeplearningdataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48c59636",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data, y_data = data[['margin', \"ebitda\", 'revenue', \"profit\"]], data[['Change']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c66c14b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = X_data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ecd5aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = y_data.to_numpy()\n",
    "temp = []\n",
    "for i in y_data:\n",
    "    if(i > 0):\n",
    "        temp.append(1)\n",
    "    else:\n",
    "        temp.append(0)\n",
    "y_data = np.array(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ae14d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor = torch.Tensor(X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a265d967",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tensor = torch.Tensor(y_data).type(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d57f22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train , y_test = train_test_split(X_tensor, y_tensor, test_size = 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94a92b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ffd7d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 4])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for i, (inputs, labels) in enumerate(train_loader): \n",
    "    print(inputs.shape)\n",
    "    print(labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb392d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(4, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2, padding=1)\n",
    "        self.fc1 = nn.Linear(32 , 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 2)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.Relu1 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(-1, 32)\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.Relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.Relu1(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "59d906b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "learning_rate = 0.1\n",
    "hidden_layers = 64\n",
    "momentum = 0.9\n",
    "num_layers = 2\n",
    "input_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0c45b271",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0 = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "76255f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn = nn.L1Loss()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params=model_0.parameters(), lr = learning_rate , momentum=momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527dce43",
   "metadata": {},
   "source": [
    "# CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7ff7f917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.6943356990814209 , Test Loss : 0.6926684975624084\n",
      "Train Loss : 0.7166134715080261 , Test Loss : 0.6901255249977112\n",
      "Train Loss : 0.6678937673568726 , Test Loss : 0.6921658515930176\n",
      "Train Loss : 0.6925491690635681 , Test Loss : 0.6924847364425659\n",
      "Train Loss : 0.6865905523300171 , Test Loss : 0.6882375478744507\n",
      "Train Loss : 0.6877681612968445 , Test Loss : 0.6865368485450745\n",
      "Train Loss : 0.6633457541465759 , Test Loss : 0.6876329779624939\n",
      "Train Loss : 0.6960287690162659 , Test Loss : 0.6929053664207458\n",
      "Train Loss : 0.6969648003578186 , Test Loss : 0.6896867752075195\n",
      "Train Loss : 0.6931434273719788 , Test Loss : 0.6930708885192871\n",
      "Train Loss : 0.6782352328300476 , Test Loss : 0.6891242861747742\n",
      "Train Loss : 0.680582582950592 , Test Loss : 0.6894286870956421\n",
      "Train Loss : 0.6859748959541321 , Test Loss : 0.690395712852478\n",
      "Train Loss : 0.6835548281669617 , Test Loss : 0.6919400691986084\n",
      "Train Loss : 0.6611807942390442 , Test Loss : 0.6903764605522156\n",
      "Train Loss : 0.6944817900657654 , Test Loss : 0.6917134523391724\n",
      "Train Loss : 0.696984052658081 , Test Loss : 0.6913062334060669\n",
      "Train Loss : 0.6754501461982727 , Test Loss : 0.6896423101425171\n",
      "Train Loss : 0.6886456608772278 , Test Loss : 0.6931434869766235\n",
      "Train Loss : 0.6755878925323486 , Test Loss : 0.6908808350563049\n",
      "Train Loss : 0.6931475400924683 , Test Loss : 0.6932843327522278\n",
      "Train Loss : 0.6931473016738892 , Test Loss : 0.69443678855896\n",
      "Train Loss : 0.6931473016738892 , Test Loss : 0.6932299137115479\n",
      "Train Loss : 0.6931473016738892 , Test Loss : 0.6934689879417419\n",
      "Train Loss : 0.6931473016738892 , Test Loss : 0.6931489109992981\n",
      "Train Loss : 0.6931473016738892 , Test Loss : 0.69312584400177\n",
      "Train Loss : 0.6808928847312927 , Test Loss : 0.6937618851661682\n",
      "Train Loss : 0.7086459398269653 , Test Loss : 0.7097482085227966\n",
      "Train Loss : 0.7609076499938965 , Test Loss : 0.6974931359291077\n",
      "Train Loss : 0.740903913974762 , Test Loss : 0.7131611108779907\n",
      "Train Loss : 0.7564026117324829 , Test Loss : 0.7264514565467834\n",
      "Train Loss : 0.6441298127174377 , Test Loss : 0.7264514565467834\n",
      "Train Loss : 0.7163952589035034 , Test Loss : 0.7260797023773193\n",
      "Train Loss : 0.7931657433509827 , Test Loss : 0.7260797023773193\n",
      "Train Loss : 0.6996357440948486 , Test Loss : 0.7288211584091187\n",
      "Train Loss : 0.6673777103424072 , Test Loss : 0.7290070652961731\n",
      "Train Loss : 0.7131509184837341 , Test Loss : 0.6931473016738892\n",
      "Train Loss : 0.6931473016738892 , Test Loss : 0.6931473016738892\n",
      "Train Loss : 0.7331546545028687 , Test Loss : 0.6931473016738892\n",
      "Train Loss : 0.6808928847312927 , Test Loss : 0.6931473016738892\n",
      "Train Loss : 0.7131510376930237 , Test Loss : 0.6931473016738892\n",
      "Train Loss : 0.720900297164917 , Test Loss : 0.6931473016738892\n",
      "Train Loss : 0.6931473016738892 , Test Loss : 0.6931473016738892\n",
      "Train Loss : 0.6931473016738892 , Test Loss : 0.6931473016738892\n",
      "Train Loss : 0.6931473016738892 , Test Loss : 0.6931473016738892\n",
      "Train Loss : 0.6808928847312927 , Test Loss : 0.6931473016738892\n",
      "Train Loss : 0.7131510376930237 , Test Loss : 0.6931473016738892\n",
      "Train Loss : 0.6808928847312927 , Test Loss : 0.6931473016738892\n",
      "Train Loss : 0.7131510376930237 , Test Loss : 0.6931473016738892\n",
      "Train Loss : 0.6931473016738892 , Test Loss : 0.6931473016738892\n",
      "Train Loss : 0.6931473016738892 , Test Loss : 0.6931473016738892\n",
      "Train Loss : 0.720900297164917 , Test Loss : 0.6931473016738892\n",
      "Train Loss : 0.6808928847312927 , Test Loss : 0.6931473016738892\n",
      "Train Loss : 0.6931473016738892 , Test Loss : 0.6931473016738892\n",
      "Train Loss : 0.6931473016738892 , Test Loss : 0.6931473016738892\n",
      "Train Loss : 0.6931473016738892 , Test Loss : 0.6931473016738892\n",
      "Train Loss : 0.6931473016738892 , Test Loss : 0.6931473016738892\n",
      "Train Loss : 0.6808928847312927 , Test Loss : 0.6931473016738892\n",
      "Train Loss : 0.6931473016738892 , Test Loss : 0.6931473016738892\n",
      "Train Loss : 0.6931473016738892 , Test Loss : 0.6931473016738892\n",
      "Train Loss : 0.6931473016738892 , Test Loss : 0.6931473016738892\n",
      "Train Loss : 0.7131510376930237 , Test Loss : 0.6931473016738892\n",
      "Train Loss : 0.7086459398269653 , Test Loss : 0.6931473016738892\n",
      "Train Loss : 0.6931473016738892 , Test Loss : 0.6931473016738892\n",
      "Train Loss : 0.6931473016738892 , Test Loss : 0.6931473016738892\n",
      "Train Loss : 0.6931473016738892 , Test Loss : 0.6931473016738892\n",
      "Train Loss : 0.6931473016738892 , Test Loss : 0.6931473016738892\n",
      "Train Loss : 0.6931473016738892 , Test Loss : 0.6931473016738892\n",
      "Train Loss : 0.6931473016738892 , Test Loss : 0.6931473016738892\n",
      "Train Loss : 0.7131510376930237 , Test Loss : 0.6931473016738892\n",
      "Train Loss : 0.6931473016738892 , Test Loss : 0.6931473016738892\n",
      "Train Loss : 0.6931473016738892 , Test Loss : 0.6931473016738892\n",
      "Train Loss : 0.6931473016738892 , Test Loss : 0.6931473016738892\n",
      "Train Loss : 0.7131509184837341 , Test Loss : 0.6931473016738892\n",
      "Train Loss : 0.6931473016738892 , Test Loss : 0.6931473016738892\n",
      "Train Loss : 0.6931473016738892 , Test Loss : 0.6931473016738892\n",
      "Train Loss : 0.6931473016738892 , Test Loss : 0.6931473016738892\n",
      "Train Loss : 0.6808928847312927 , Test Loss : 0.6931473016738892\n",
      "Train Loss : 0.7331546545028687 , Test Loss : 0.6931473016738892\n",
      "Train Loss : 0.6808928847312927 , Test Loss : 0.6931473016738892\n",
      "Train Loss : 0.6931473016738892 , Test Loss : 0.6931473016738892\n",
      "Train Loss : 0.7086459398269653 , Test Loss : 0.7003433704376221\n",
      "Train Loss : 0.6808928847312927 , Test Loss : 0.7003433704376221\n",
      "Train Loss : 0.7531583905220032 , Test Loss : 0.7003433704376221\n",
      "Train Loss : 0.7008966207504272 , Test Loss : 0.7003433704376221\n",
      "Train Loss : 0.7531583905220032 , Test Loss : 0.7003433704376221\n",
      "Train Loss : 0.6808928847312927 , Test Loss : 0.7003433704376221\n",
      "Train Loss : 0.6931473016738892 , Test Loss : 0.7003433704376221\n",
      "Train Loss : 0.720900297164917 , Test Loss : 0.7003433704376221\n",
      "Train Loss : 0.7331547141075134 , Test Loss : 0.7003433704376221\n",
      "Train Loss : 0.7008966207504272 , Test Loss : 0.7003433704376221\n",
      "Train Loss : 0.7609077095985413 , Test Loss : 0.7003433704376221\n",
      "Train Loss : 0.688642144203186 , Test Loss : 0.7003433704376221\n",
      "Train Loss : 0.7131509184837341 , Test Loss : 0.7003433704376221\n",
      "Train Loss : 0.6886422038078308 , Test Loss : 0.7003433704376221\n",
      "Train Loss : 0.7131509184837341 , Test Loss : 0.7003433704376221\n",
      "Train Loss : 0.7209001779556274 , Test Loss : 0.7003433704376221\n",
      "Train Loss : 0.6686384677886963 , Test Loss : 0.7003433704376221\n",
      "Train Loss : 0.7331546545028687 , Test Loss : 0.7003433704376221\n",
      "Train Loss : 0.7008966207504272 , Test Loss : 0.7003433704376221\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i, (inputs, labels) in enumerate(train_loader): \n",
    "        #model train mode\n",
    "        model_0.train()\n",
    "\n",
    "        # Forward pass\n",
    "        y_preds = model_0(inputs.unsqueeze(dim=2))\n",
    "        #calculate the loss \n",
    "        \n",
    "        loss = loss_fn(y_preds, labels)\n",
    "\n",
    "        #optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #back propagation \n",
    "        loss.backward()\n",
    "\n",
    "        #gradient descent \n",
    "        optimizer.step()\n",
    "\n",
    "    if(epoch%10 == 0):\n",
    "        model_0.eval()\n",
    "        with torch.inference_mode():\n",
    "            y_test_pred = model_0(X_test.unsqueeze(dim=2))\n",
    "            loss_test = loss_fn(y_test_pred,y_test)\n",
    "\n",
    "        print(f\"Train Loss : {loss.item()} , Test Loss : {loss_test.item()}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b57dd92",
   "metadata": {},
   "source": [
    " # RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "24f54089",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNNModel, self).__init__() \n",
    "        self.rnn = nn.RNN(4, 256)\n",
    "        self.l1 = nn.Linear(in_features = 256, out_features = 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(in_features = 128 , out_features = 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x,y = self.rnn(x)\n",
    "#         print(y.shape)\n",
    "        x = self.l1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.l2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e055658e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = RNNModel()\n",
    "epochs = 100\n",
    "learning_rate = 0.01\n",
    "hidden_layers = 64\n",
    "momentum = 0.9\n",
    "num_layers = 2\n",
    "input_size = 4\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params=model_1.parameters(), lr = learning_rate , momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0ddf0897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.7044835686683655 , Test Loss : 0.6919167041778564\n",
      "Train Loss : 0.6837254762649536 , Test Loss : 0.6929894089698792\n",
      "Train Loss : 0.7006697654724121 , Test Loss : 0.6928457021713257\n",
      "Train Loss : 0.675035834312439 , Test Loss : 0.6942077875137329\n",
      "Train Loss : 0.7082319855690002 , Test Loss : 0.6922135353088379\n",
      "Train Loss : 0.6997717618942261 , Test Loss : 0.6928080916404724\n",
      "Train Loss : 0.6889241933822632 , Test Loss : 0.6937897801399231\n",
      "Train Loss : 0.6721938252449036 , Test Loss : 0.6941248178482056\n",
      "Train Loss : 0.6872169971466064 , Test Loss : 0.6912552118301392\n",
      "Train Loss : 0.689087450504303 , Test Loss : 0.6948478817939758\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i, (inputs, labels) in enumerate(train_loader): \n",
    "        #model train mode\n",
    "        model_0.train()\n",
    "\n",
    "        # Forward pass\n",
    "        y_preds = model_1(inputs)\n",
    "        #calculate the loss \n",
    "#         print(y_preds.shape)\n",
    "#         break\n",
    "        loss = loss_fn(y_preds, labels)\n",
    "\n",
    "        #optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #back propagation \n",
    "        loss.backward()\n",
    "\n",
    "        #gradient descent \n",
    "        optimizer.step()\n",
    "#     break\n",
    "    if(epoch%10 == 0):\n",
    "        model_0.eval()\n",
    "        with torch.inference_mode():\n",
    "            y_test_pred = model_1(X_test)\n",
    "            loss_test = loss_fn(y_test_pred,y_test)\n",
    "\n",
    "        print(f\"Train Loss : {loss.item()} , Test Loss : {loss_test.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
